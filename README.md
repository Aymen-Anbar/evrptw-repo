# Uncertainty-Aware Deep Reinforcement Learning for Sustainable Electric Vehicle Routing

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0](https://img.shields.io/badge/pytorch-2.0-orange.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Paper](https://img.shields.io/badge/paper-Elsevier-red.svg)](#citation)

Official code repository for:

> **Uncertainty-Aware Deep Reinforcement Learning for Sustainable Electric Vehicle Routing: A Hybrid Optimization Framework**  
> Aymen Jalil Abdulelah, Emrullah SonuÃ§, Esam Taha Yassen, Ahmeed Suliman Farhan, Ali Al-kubaisi, Ahmed Shamil Mustafa  
> *Transportation Research Part D: Transport and Environment*, 2025

---

## Abstract

Urban freight logistics accounts for approximately 20% of transport-related greenhouse gas emissions in cities worldwide, yet electric vehicle adoption faces critical operational barriers: limited range, charging infrastructure constraints, and energy consumption uncertainty. Existing routing approaches fail to address a fundamental challenge: energy consumption exhibits 30â€“40% variability due to traffic, weather, and payload, yet deterministic planning methods cannot quantify vehicle stranding risk or provide formal feasibility guarantees.

We address the Electric Vehicle Routing Problem with Time Windows (EVRPTW) under operational uncertainty through probabilistic energy forecasting integrated with hybrid optimization. Our framework employs bidirectional LSTM networks for calibrated uncertainty quantification (**94.7% empirical coverage** for 95% confidence intervals), enabling formal chance-constrained battery feasibility. Graph neural networks encode battery-aware spatial-temporal dependencies, while Proximal Policy Optimization with optional mixed-integer programming refinement balances real-time adaptability (**0.3â€“0.8 s inference**) with offline solution quality.

Extensive validation on 30 large-scale benchmark instances demonstrates **12.3% cost reduction** versus state-of-the-art matheuristics, with **18.7% performance gains** under dynamic operational conditions. Real-world deployment with a 25-vehicle electric fleet over 13 weeks validates computational findings: approximately â‚¬143,000 in projected annual savings, 96.7% on-time delivery, 15.5% energy reduction, and 96% planning time reduction, translating to approximately **19 tons of COâ‚‚ avoided annually**.

---

## Framework Overview

```
Problem Instance (I)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   H^(L) (once)   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GAT Encoder  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚  LSTM Predictor  â”‚
â”‚  (4 layers,   â”‚                  â”‚  (BiLSTM, 64h)   â”‚
â”‚   8 heads)    â”‚                  â”‚  (Î¼_E, ÏƒÂ²_E)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚ per-step oracle
                                            â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚   PPO Policy    â”‚
                                   â”‚  Ï€_Î¸(a | s)     â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â–¼               â–¼               â–¼
                     Chance Constraint  Neural Solution  Value Network
                     Check (95% CI)     (DRL-Pure)      (Advantage)
                            â”‚
                            â–¼ (if time budget allows)
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ MILP Solver â”‚  â†’ Refined Solution R*
                     â”‚  (Gurobi)   â”‚     (DRL-Hybrid, +3.2%)
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The framework operates in **four phases**:
- **Phase 1 â€” Offline Pre-training**: 35,000 episodes with curriculum learning (10â†’100 customers)
- **Phase 2 â€” Transfer Learning**: 400â€“600 local routes, 3â€“5 hours adaptation
- **Phase 3 â€” Online Routing**: 0.3â€“0.8 s inference with chance-constrained feasibility
- **Phase 4 â€” Optional MILP Refinement**: 3â€“4% additional quality, 300â€“1200 s budget

---

## Key Results

### Static Benchmark Performance (30 instances, 60â€“100 customers)

| Method | Cost | Î” RKS | Time (s) | Feasibility | Vehicles | Energy |
|--------|------|-------|----------|-------------|----------|--------|
| RKS (SOTA matheuristic) | 14,856 | â€” | 2,847 | 89.2% | 14.8 | 101.2 |
| AM (Kool et al., 2019) | 16,234 | +9.3% | 0.4 | 85.6% | 15.9 | 106.8 |
| POMO (Kwon et al., 2020) | 15,678 | +5.5% | 1.2 | 87.1% | 15.2 | 103.4 |
| MVMoE (Zhou et al., 2024) | 14,287 | âˆ’3.8% | 2.1 | 90.1% | 14.3 | 99.8 |
| DACT (Ma et al., 2021) | 14,456 | âˆ’2.7% | 8.4 | 88.7% | 14.6 | 100.3 |
| **DRL-Pure (ours)** | **13,398** | **âˆ’9.8%***  | **0.8** | 92.4% | 14.1 | 98.7 |
| **DRL-Hybrid (ours)** | **13,026** | **âˆ’12.3%*** | 308 | **93.8%** | **13.9** | **96.3** |

*p < 0.001, Wilcoxon signed-rank test vs. RKS. Five seeds per instance.*

### Dynamic Operational Performance (100 episodes, Â±20% demand, Â±15% energy noise)

| Method | Cost | Re-routes | On-time | Replan Time | Stranded |
|--------|------|-----------|---------|-------------|----------|
| RKS | 18,234 | 8.7 | 84.2% | 2,318 s | 3.2 |
| MVMoE | 16,892 | 5.4 | 89.7% | 45 s | 1.8 |
| DACT | 16,456 | 6.1 | 88.3% | 127 s | 2.1 |
| **DRL-Hybrid (ours)** | **14,823** | **3.2** | **96.7%** | **6.2 s** | **0.4** |

### Ablation Study (12 instances, 20â€“40 customers)

| Configuration | Cost | Î” Full |
|---------------|------|--------|
| MLP encoder + deterministic energy | 11,892 | +12.6% |
| GAT encoder + deterministic energy | 11,234 | +7.4% |
| GAT + uncertainty-aware energy | 10,823 | +3.9% |
| Full model without MILP | 10,456 | +0.7% |
| **Full model â€” DRL-Hybrid** | **10,389** | â€” |

### Real-World Deployment (25 vehicles, 13 weeks)

| Metric | Baseline | Week 1â€“4 | Week 5â€“8 | Week 9â€“13 | Improvement |
|--------|----------|----------|----------|-----------|-------------|
| Daily cost (â‚¬) | 2,847 | 2,643 | 2,489 | 2,367 | âˆ’16.9% |
| On-time delivery | 91.3% | 94.2% | 96.5% | 98.1% | +6.8 pp |
| Energy (kWh) | 1,456 | 1,342 | 1,294 | 1,231 | âˆ’15.5% |
| Planning time | 45 min | 2.3 min | 2.1 min | 1.8 min | âˆ’96.0% |

---

## Repository Structure

```
uncertainty-aware-evrptw/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ gat_encoder.py       # Graph Attention Network (4 layers, 8 heads, d=128)
â”‚   â”‚   â”œâ”€â”€ lstm_predictor.py    # Bidirectional LSTM energy predictor
â”‚   â”‚   â”œâ”€â”€ ppo_policy.py        # PPO policy network with clipped surrogate
â”‚   â”‚   â””â”€â”€ value_network.py     # Value network for advantage estimation
â”‚   â”œâ”€â”€ environment/
â”‚   â”‚   â”œâ”€â”€ evrptw_env.py        # EVRPTW MDP environment
â”‚   â”‚   â””â”€â”€ energy_model.py      # Stochastic energy consumption model
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py           # Main training loop (Phase 1)
â”‚   â”‚   â””â”€â”€ curriculum.py        # Curriculum learning scheduler
â”‚   â”œâ”€â”€ optimization/
â”‚   â”‚   â”œâ”€â”€ milp_solver.py       # Gurobi MILP warm-start refinement (Phase 4)
â”‚   â”‚   â””â”€â”€ chance_constraint.py # Chance-constrained feasibility checker
â”‚   â”œâ”€â”€ transfer/
â”‚   â”‚   â””â”€â”€ transfer_learning.py # Three-stage transfer protocol (Phases 2â€“3)
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ data_loader.py       # Benchmark instance loader
â”‚       â””â”€â”€ metrics.py           # Evaluation metrics
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                 # Launch Phase 1 pre-training
â”‚   â”œâ”€â”€ evaluate.py              # Benchmark evaluation with baselines
â”‚   â”œâ”€â”€ transfer.py              # Run transfer learning on new domain
â”‚   â””â”€â”€ reproduce_results.sh     # Reproduce all paper results
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml             # Default hyperparameters
â”‚   â””â”€â”€ transfer.yaml            # Transfer learning configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ benchmarks/              # Solomon-extended EVRPTW instances
â”œâ”€â”€ pretrained/
â”‚   â””â”€â”€ README.md                # Instructions for downloading weights
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_models.py           # Unit tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ LICENSE
```

---

## Installation

```bash
# Clone the repository
git clone https://github.com/aymenjalil/uncertainty-aware-evrptw.git
cd uncertainty-aware-evrptw

# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate          # Linux / macOS
# venv\Scripts\activate           # Windows

# Install dependencies
pip install -r requirements.txt

# Install the package in editable mode
pip install -e .
```

**Requirements:** Python 3.9+, PyTorch 2.0+, CUDA 11.8+ (recommended).  
**Optional:** Gurobi 10.0 with a valid licence for MILP refinement (Phase 4). A free academic licence is available at [gurobi.com/academia](https://www.gurobi.com/academia/academic-program-and-licenses/).

---

## Quick Start

### 1 â€” Pre-training (Phase 1)

```bash
python scripts/train.py \
    --config configs/default.yaml \
    --output_dir runs/pretrain \
    --gpus 1
```

Training takes approximately 72 hours on a single NVIDIA A100 (40 GB).  
Curriculum learning progresses automatically from 10 to 100 customers over 35,000 episodes.

### 2 â€” Evaluation on Benchmarks

```bash
python scripts/evaluate.py \
    --checkpoint runs/pretrain/best_model.pt \
    --benchmark data/benchmarks/ \
    --mode hybrid \          # 'pure' for DRL-Pure, 'hybrid' for DRL-Hybrid
    --seeds 0 42 123 456 789
```

### 3 â€” Transfer Learning to a New Domain (Phase 2)

```bash
python scripts/transfer.py \
    --checkpoint runs/pretrain/best_model.pt \
    --local_routes data/your_domain/routes.json \
    --config configs/transfer.yaml \
    --output_dir runs/transfer
```

Transfer takes 3â€“5 hours on the same hardware using 400â€“600 local routes.

### 4 â€” Reproduce All Paper Results

```bash
bash scripts/reproduce_results.sh
```

---

## Configuration

Key hyperparameters in `configs/default.yaml`:

```yaml
# Model architecture
gat:
  num_layers: 4
  num_heads: 8
  embedding_dim: 128
  dropout: 0.1

lstm:
  hidden_dim: 64
  num_layers: 2
  bidirectional: true
  input_dim: 24          # 24-dimensional segment feature vector

ppo:
  learning_rate: 3.0e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  entropy_coeff: 0.01

# Training
training:
  total_episodes: 35000
  batch_size: 64
  curriculum_start: 10   # customers
  curriculum_end: 100

# MILP refinement
milp:
  time_limit: 1200       # seconds
  mip_gap: 0.01

# Chance constraint
feasibility:
  confidence_level: 0.95  # alpha = 0.95, Phi^-1(0.95) â‰ˆ 1.645
  min_battery_reserve: 0.05  # 5% minimum SOC
```

---

## Benchmark Instances

We evaluate on 30 large-scale instances (60â€“100 customers) extended from Solomon benchmarks with:
- Urban charging infrastructure density: 0.15 stations/kmÂ²
- Heterogeneous charging rates: 50â€“150 kW
- Battery capacity: 150 kWh | Payload: 3 tons
- Energy CV: 30â€“40% (calibrated to real-world EV delivery data)

Download the benchmark instances:
```bash
python scripts/download_benchmarks.py --output data/benchmarks/
```

---

## Pre-trained Models

Pre-trained model weights are available for download:

```bash
python scripts/download_pretrained.py --output pretrained/
```

| Model | Description | Size |
|-------|-------------|------|
| `gat_lstm_ppo_35k.pt` | Full pre-trained framework | ~45 MB |
| `lstm_predictor_50k.pt` | LSTM energy predictor only | ~2 MB |

See `pretrained/README.md` for details.

---

## Citation

If you use this code in your research, please cite:

```bibtex
@article{abdulelah2025uncertainty,
  title   = {Uncertainty-Aware Deep Reinforcement Learning for Sustainable
             Electric Vehicle Routing: A Hybrid Optimization Framework},
  author  = {Abdulelah, Aymen Jalil and Sonu\c{c}, Emrullah and
             Yassen, Esam Taha and Farhan, Ahmeed Suliman and
             Al-kubaisi, Ali and Mustafa, Ahmed Shamil},
  journal = {Transportation Research Part D: Transport and Environment},
  year    = {2025},
  publisher = {Elsevier}
}
```

---

## License

This project is licensed under the MIT License â€” see [LICENSE](LICENSE) for details.

---

## Contact

**Aymen Jalil Abdulelah** (Corresponding Author)  
Electronic Computer Center, University of Anbar, Ramadi, Iraq  
ğŸ“§ ayman.ja90@uoanbar.edu.iq
